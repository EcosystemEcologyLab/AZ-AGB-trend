---
title: "Running this workflow on the HPC"
format: html
editor: visual
---

At one point, I did get this workflow running on the UA HPC with each target running on distributed SLURM jobs with larger memory allocations for heavier-duty targets.
The top of such a `_targets.R` file would look something like this:

``` r
# Load packages required to define the pipeline:
library(targets)
library(tarchetypes)
library(geotargets)
library(rlang)
library(crew)
library(crew.cluster)

# Detect whether you're on HPC & NOT with an Open OnDemand session (which cannot submit SLURM jobs)
slurm_host <- Sys.getenv("SLURM_SUBMIT_HOST")
hpc <- grepl("hpc\\.arizona\\.edu", slurm_host) & !grepl("ood", slurm_host)

# Define two (or more) controllers for the HPC
# one for "light" tasks that require less RAM
controller_light <- crew.cluster::crew_controller_slurm(
  name = "hpc_light",
  workers = 4,
  seconds_idle = 300, # time until workers are shut down after idle
  garbage_collection = TRUE, # run garbage collection between tasks
  launch_max = 5L, # number of unproductive launched workers until error
  slurm_partition = "standard",
  slurm_time_minutes = 1200, #wall time for each worker
  slurm_log_output = "logs/crew_log_%A.out",
  slurm_log_error = "logs/crew_log_%A.err",
  slurm_memory_gigabytes_per_cpu = 5,
  slurm_cpus_per_task = 4, #total 20gb RAM
  script_lines = c(
    "#SBATCH --account davidjpmoore",
    "module load gdal/3.8.5 R/4.3 eigen/3.4.0 netcdf/4.7.1"
    #add additional lines to the SLURM job script as necessary here
  )
)

# and one that uses the high memory nodes
controller_himem <- crew.cluster::crew_controller_slurm(
  name = "hpc_himem",
  workers = 2,
  seconds_idle = 300, # time until workers are shut down after idle
  garbage_collection = TRUE, # run garbage collection between tasks
  launch_max = 5L, # number of unproductive launched workers until error
  slurm_partition = "standard",
  slurm_time_minutes = 1200, #wall time for each worker
  slurm_log_output = "logs/crew_log_%A.out",
  slurm_log_error = "logs/crew_log_%A.err",
  slurm_memory_gigabytes_per_cpu = 32,
  slurm_cpus_per_task = 3, # total 96gb RAM
  script_lines = c(
    "#SBATCH --account davidjpmoore",
    "#SBATCH --constraint=hi_mem", #use high-memory nodes
    "module load gdal/3.8.5 R/4.3 eigen/3.4.0 netcdf/4.7.1"
    #add additional lines to the SLURM job script as necessary here
  )
)

# and define a local controller to fall back on when not on the HPC or using the HPC with Open OnDemand
controller_local <- 
  crew::crew_controller_local(
    name = "local", 
    workers = 4, 
    seconds_idle = 60,
    local_log_directory = "logs"
  )

# Set target options:
tar_option_set(
  packages = c("ncdf4", "terra", "fs", "purrr", "ncdf4", "car", "dplyr"), # Packages that your targets need for their tasks.
  controller = crew_controller_group(controller_local, controller_heavy, controller_light),
  #if on HPC use "hpc_light" controller by default, otherwise use "local"
  resources = tar_resources(
    crew = tar_resources_crew(controller = ifelse(hpc, "hpc_light", "local"))
  ),
  # improve memory performance
  memory = "transient", 
  garbage_collection = TRUE,
  # allow workers to access _targets/ store directly
  storage = "worker",
  retrieval = "worker"
)
```

This creates a crew controller group with multiple controller options, each with a name.
Then, in each target, you can specify which controller to use with `resources = tar_resources(crew = tar_resources_crew(controller = "controller name"))` .
For example, a "heavy duty" target might look like this:

``` r
tar_terra_rast(
    ltgnn_agb,
    read_clean_ltgnn(ltgnn_files, az),
    resources = tar_resources(
      crew = tar_resources_crew(controller = ifelse(hpc, "hpc_heavy", "local"))
    )
  )
```

## Running the pipeline

There are a few options for actually running the pipeline on the HPC.

1.  From RStudio with Open OnDemand
2.  From the commandline using an interactive session on the HPC
3.  By submitting a run.sh script (example also in the `notes/` folder)

### Open OnDemand

Go to <https://ood.hpc.arizona.edu/pun/sys/dashboard/apps/index> and choose RStudio Server.
Customize your session to use multiple cores and set the PI Group to `davidjpmoore`.
Launch the session and open this project on the HPC at `/groups/davidjpmoore/AZ-AGB-trend` .
Run `targets::tar_make(as_job = TRUE)` to kick off the pipeline.

### Interactive session

SSH into the HPC, navigate to this project, and request an interactive session with `interactive -a davidjpmoore -t 05:00:00 -n` where the time is how long you'd like the session to go and the `-n 1` indicates one core.
Then load the relevant modules with `module load gdal/3.8.5 R/4.3 eigen/3.4.0 netcdf/4.7.1`.
Launch R with `R` and run `targets::tar_make()` to kick off the pipeline.
The major upside to this is you can watch the pipeline work and it uses SLURM jobs for workers (unlike the OOD way).
The major downside is you need to keep the interactive session running until the pipeline finishes, and you probably don't know ahead of time how long that will take

### Submitting run.sh

SSH into the HPC, navigate to this project, and run `sbatch run.sh`.
You can watch progress by occasionally running `squeue -u yourusername` to see the workers launch and you can peek at the `logs/` folder.
You can find the most recently modified files with something like `ls -lt | head -n 5` and then you can read the logs with `cat targets_main_9814039.out` (or whatever filename you want to read).
